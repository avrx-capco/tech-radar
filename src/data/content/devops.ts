const techContent = [
  {
    technology: "GitHub",
    docsLink: "https://docs.github.com/en",
    intro:
      "GitHub is a cloud-based hosting service that lets you manage Git repositories, Github is ubiquitously used in open source sofrware teams and very prominent within Enterprise, Github provided web based features, team manament and other capbilties on top of Git.",
    content: [
      {
        name: "",
        intro: "",
        data: [],
      },
    ],
    examples: [],
    reference: [],
  },
  {
    technology: "CircleCI",
    docsLink: "https://circleci.com/docs/",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Terraform",
    docsLink: "https://www.terraform.io/docs/index.html",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Vault",
    docsLink: "https://www.vaultproject.io/docs",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "SonarQube",
    docsLink: "https://docs.sonarqube.org/",
    intro:
      "SonarQube empowers all developers to write cleaner and safer code. It is an established name for static code analysis and its integration with Jenkins makes it much easier for developers to learn from the thousands of pre-configured automated Static Code Analysis rules. Sonar helps in protecting the code on multiple fronts and making your team to learn from the industry approved language specific rules. SonarQube has a huge community of more than 200K dev teams.",
    content: [
      {
        name: "",
        intro:
          "SonarQube (formerly Sonar) is an open-source platform developed by Sonar Source for continuous inspection of code quality to perform automatic reviews with static analysis of code to detect bugs, code smells, and security vulnerabilities on 20+ programming languages. SonarQube offers reports on duplicated code, coding standards, unit tests, code coverage, code complexity, comments, bugs, and security vulnerabilities. ",
        data: [],
      },
      {
        name: "",
        intro:
          "SonarQube provides fully automated analysis and integration with various build tools like  Maven, Ant, Gradle, MS Build and Continuous Integration tools like (Atlassian Bamboo, Jenkins etc.). SonarQube supports the majority of the industry dominating programming languages and it works with multi language projects as well. ",
        data: [],
      },
      {
        name: "",
        intro:
          "With SonarQube static analysis you have one place to measure the Reliability, Security, and Maintainability of all the languages in your project, and all the projects in your sphere.",
        data: [],
      },
      {
        name: "SonarQube Benefits",
        intro: "",
        data: [
          {
            name: "",
            description:
              "1. Code Quality<ul><li>Release Quality Code</li><li>Maintainability</li></ul>2. Code Security<ul><li>Security Analysis</li><li>OWASP Top 10</li></ul>3. Detect Technical Debt Early stage<br/><br/>4. Community Support<br/><br/>5. Market Place with reusable plugins for e.g., Build Breaker.<br/><br/>6. Continuous monitor on the Code Coverage",
          },
        ],
      },
      {
        name: "SonarQube Advantages",
        intro: "",
        data: [
          {
            name: "Integration with Industry Standard CI & Version Control Tool",
            description:
              "<ul><li>Jenkins, Azure DevOps, Team City </li><li>Git, Bit Bucket, GitLab </li></ul>",
          },
          {
            name: "Build Frameworks Supported",
            description:
              "A Majority of the major build tools are supported by SonarQube, so you don’t need to change your existing setup to introduce SonarQube. ",
          },
          {
            name: "Deploy with Confidence",
            description:
              "Having configured the Quality Gates within your CI & Version Control System, you can rest assured of the quality of the code before it reaches to your customers. This automation is very effective than compared to traditional manual human code review. With SonarQube, those code reviews may now focus more on the functional aspect of the code. ",
          },
        ],
      },
    ],
    examples: [],
    reference: [],
  },
  {
    technology: "Elastic",
    docsLink: "https://www.elastic.co/guide/index.html",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Bitbucket",
    docsLink: "https://bitbucket.org/product/guides",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Jenkins",
    docsLink: "https://www.jenkins.io/doc/",
    intro:
      "Jenkins is a leading open source automation server. It provides an extensive marketplace of plugins to support building, deploying and automating any project as per your specific needs.",
    content: [
      {
        name: "",
        intro:
          "The Jenkins project was started in 2004 (originally called Hudson) by Kohsuke Kawaguchi, while he worked for Sun Microsystems. The intent behind creating this tool was to ensure the continuous integration happens automatically and every incremental project change can be validated against stability with the existing code.       Originally created as a CI (Continuous Integration) tool, Jenkins evolved to a CD (Continuous Delivery) tool and it orchestrates the entire software delivery pipeline.",
        data: [],
      },
      {
        name: "",
        intro:
          "Jenkins is the most widely adopted solution for continuous delivery, thanks to its extensibility and a vibrant, active community. The Jenkins community offers thousands of plugins that enable Jenkins to integrate with virtually any tool, including all of the best-of-breed solutions used throughout the continuous delivery process. Jenkins continues to grow as the dominant solution for software process automation, continuous integration and continuous delivery. ",
        data: [],
      },
      {
        name: "Jenkins Capabilities",
        intro: "",
        data: [
          {
            name: "Easy Installation",
            description:
              "Jenkins is a platform-agnostic, self-contained Java-based program, ready to run with packages for Windows, Mac OS, and Unix-like operating systems.",
          },
          {
            name: "Easy Configuration",
            description:
              "Jenkins is easily set up and configured using its web interface, featuring error checks and a built-in help function.",
          },
          {
            name: "Available Plugins",
            description:
              "There are thousands of plugins available in the Update Center, integrating with every tool in the CI and CD toolchain.",
          },
          {
            name: "Extensible",
            description:
              "Jenkins can be extended by means of its plugin architecture, providing nearly endless possibilities for what it can do.",
          },
          {
            name: "Free Open Source",
            description:
              "Jenkins is an open-source resource backed by heavy community support. ",
          },
          {
            name: "Easy Distribution",
            description:
              "Jenkins can easily distribute work across multiple machines for faster builds, tests, and deployments across multiple platforms.",
          },
        ],
      },
      {
        name: "Jenkins Advantages",
        intro: "",
        data: [
          {
            name: "Adoptable",
            description:
              "Jenkins adopts to the ongoing market trend and the community ensures the right set of plugins are available to provide integration with the technology boom. For example Jenkins can be deployed as a Stand-Alone Server, as a Docker Container or, as a Docker Compose setup. Similarly, it can have a static number of worker nodes, or on demand auto scaled workers launched in any Cloud platform like AWS, GCP.",
          },
          {
            name: "Customizable",
            description:
              "Jenkins can be used based on needs. For simple projects, you can use it as a combination of Upstream Downstream builds to design an integration flow. For more complex projects you can design extensive pipelines for detailed executions.",
          },
          {
            name: "Align with Agile Dev & DevOps",
            description:
              "Jenkins CI/CD pipeline aligns with the Agile deliveries and Agile DevOps mindset which is a must have in today’s ever-changing environment. A great CI/CD pipeline ensures that requirements like quality, code coverage, code analysis, stability, performance, end to end testing etc can be set up and executed once and repeated forever. This also impacts the time to market which is a game changer in today’s world.",
          },
          {
            name: "Process Improvement",
            description:
              "With seamless integration with Source Control Systems like GitHub, Jenkins improves the overall Pull Request based development experience. Easy integration with Git’s Actions and Workflows, push / pull event-based integration. Branching strategies can be implemented very well to meet the specific requirements.",
          },
          {
            name: "Inline Reporting",
            description:
              "Jenkins provides great reporting tools to analyse the Test Results, Cucumber Selenium based Functional Test Results, JMeter, Gatling based performance test results, SonarQube / Code Style based code analysis / coverage reports. This ensure that the team don’t need to understand different tools for each requirement. Once they are acquainted with Jenkins, it’s all good forever",
          },
        ],
      },
    ],
    examples: [],
    reference: [],
  },
  {
    technology: "TeamCity",
    docsLink:
      "https://www.jetbrains.com/help/teamcity/teamcity-documentation.html",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Istio",
    docsLink: "https://istio.io/latest/docs/",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Ansible",
    docsLink: "https://docs.ansible.com/",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Packer",
    docsLink: "https://www.packer.io/docs",
    intro:
      "Packer lets you create identical machine images for multiple platforms from a single source configuration. A common use case is creating golden images for organizations to use in cloud infrastructure.",
    content: [
      {
        name: "",
        intro:
          "A machine image is a single static unit that contains a pre-configured operating system and installed software which is used to quickly create new running machines. Machine image formats change for each platform. Some examples include AMIs for EC2, VMDK/VMX files for VMware, OVF exports for VirtualBox, etc.",
        data: [],
      },
      {
        name: "Packer Advantages",
        intro: "",
        data: [
          {
            name: "Super fast infrastructure deployment",
            description:
              "Packer images allow you to launch completely provisioned and configured machines in seconds, rather than several minutes or hours. This benefits not only production, but development as well, since development virtual machines can also be launched in seconds, without waiting for a typically much longer provisioning time.",
          },
          {
            name: "Multi-provider portability",
            description:
              "Because Packer creates identical images for multiple platforms, you can run production in AWS, staging/QA in a private cloud like OpenStack, and development in desktop virtualization solutions such as VMware or VirtualBox. Each environment is running an identical machine image, giving ultimate portability.",
          },
          {
            name: "Improved stability",
            description:
              "Packer installs and configures all the software for a machine at the time the image is built. If there are bugs in these scripts, they'll be caught early, rather than several minutes after a machine is launched.",
          },
          {
            name: "Greater testability",
            description:
              "After a machine image is built, that machine image can be quickly launched and smoke tested to verify that things appear to be working. If they are, you can be confident that any other machines launched from that image will function properly.",
          },
        ],
      },
      {
        name: "Packer Use Cases",
        intro: "",
        data: [
          {
            name: "Continuous Delivery",
            description:
              "Packer is lightweight, portable, and command-line driven. This makes it the perfect tool to put in the middle of your continuous delivery pipeline. Packer can be used to generate new machine images for multiple platforms on every change to Chef/Puppet. As part of this pipeline, the newly created images can then be launched and tested, verifying the infrastructure changes work. If the tests pass, you can be confident that the image will work when deployed. This brings a new level of stability and testability to infrastructure changes.",
          },
          {
            name: "Dev/Prod Parity",
            description:
              "Packer helps keep development, staging, and production as similar as possible. Packer can be used to generate images for multiple platforms at the same time. So if you use AWS for production and VMware (perhaps with Vagrant) for development, you can generate both an AMI and a VMware machine using Packer at the same time from the same template.",
          },
          {
            name: "Appliance/Demo Creation",
            description:
              "Since Packer creates consistent images for multiple platforms in parallel, it is perfect for creating appliances and disposable product demos. As your software changes, you can automatically create appliances with the software pre-installed. Potential users can then get started with your software by deploying it to the environment of their choice.",
          },
        ],
      },
    ],
    examples: [],
    reference: ["https://developer.hashicorp.com/packer/docs"],
  },
  {
    technology: "Twistlock",
    docsLink: "https://docs.twistlock.com/docs/",
    intro:
      "TwistLock is a container security platform designed to provide DevOps teams with the tools they need to protect their containerized applications. TwistLock provides a suite of security tools including vulnerability scanning, runtime protection, and compliance monitoring. The platform also offers integration with popular CI/CD systems, allowing teams to automate security into their DevOps pipelines.",
    content: [
      {
        name: "Twistlock Capabilities",
        intro: "",
        data: [
          {
            name: "Vulnerability scanning",
            description:
              "Identifies potential risks and weaknesses in your container environment.",
          },
          {
            name: "Runtime protection",
            description:
              "Monitors and enforces security policies in real time to protect against malicious activities.",
          },
          {
            name: "Compliance and auditing",
            description:
              "Ensures compliance with applicable industry standards and regulations.",
          },
          {
            name: "Secure DevOps automation",
            description:
              "Automates security processes and reduces the risk of human error.",
          },
          {
            name: "Advanced analytics and machine learning-powered threat detection",
            description: "Quickly detect and respond to threats.",
          },
          {
            name: "Container management and monitoring",
            description:
              "Monitors and manages containerized applications and workloads.",
          },
        ],
      },
      {
        name: "Twistlock Advantages",
        intro: "",
        data: [
          {
            name: "Automated Security",
            description:
              "Twistlock automates the process of security and regulatory compliance, with real-time vulnerability scanning for container images and cloud native workloads.",
          },
          {
            name: "Comprehensive Visibility",
            description:
              "Twistlock provides end-to-end visibility and control across the cloud native stack.",
          },
          {
            name: "Cloud Native Protection",
            description:
              "Twistlock provides robust protection across the entire cloud native stack, including containers, serverless functions, and Kubernetes.",
          },
          {
            name: "Simplified Compliance",
            description:
              "Twistlock simplifies compliance and governance with built-in compliance checks and automated policy enforcement.",
          },
          {
            name: "Automated Incident Response",
            description:
              "Twistlock provides automated response and investigation tools to help identify and mitigate potential security incidents quickly and accurately.",
          },
        ],
      },
    ],
    examples: [],
    reference: ["https://docs.twistlock.com/docs/"],
  },
  {
    technology: "Kiali",
    docsLink: "https://kiali.io/documentation/",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "ConcourseCl",
    docsLink: "https://concourse-ci.org/docs.html",
    intro: "Content coming soon.",
    content: [],
    examples: [],
    reference: [],
  },
  {
    technology: "Consul",
    docsLink: "https://developer.hashicorp.com/consul",
    intro:
      "Consul is a service networking solution that enables organisations and teams to manage secure network connectivity between services and across multi-cloud environments and runtimes. The features that Consul offers are  autoamted service discovery, enabling zero trust netork security, identity-based authorization, L7 traffic management, and service-to-service encryption.",
    content: [
      {
        name: "Consul Features and Architecture",
        intro: "",
        data: [
          {
            name: "Service Discovery",
            description:
              "Consul utilises a service registry and allows automated service discovery via DNS and HTTP, Consul will help teams to track the health statuses of services in real time.",
          },
          {
            name: "Network security",
            description:
              "Consul ensure that all service to service communications are authorized and encrypted using mutual Transport Layer Security (TLS). ",
          },
          {
            name: "Platforms",
            description:
              "Consul can integarte with platforms such as Nomad, Vautl, AWS ECS and AWS Lambda",
          },
          {
            name: "Access Control",
            description:
              "Users of Consul are able to configure Access Control Lists (ACLS) to authenitcate requests and authorize access to resoources, using roles, policies and identity to provide inter services security.",
          },
        ],
      },
    ],
    examples: [],
    reference: [
      "https://www.consul.io/",
      "https://www.hashicorp.com/products/consul",
    ],
  },
  {
    technology: "Docker",
    docsLink: "https://docs.docker.com/",
    intro:
      "Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. The service has both free and premium tiers. The software that hosts the containers is called Docker Engine. It was first started in 2013 and is developed by Docker, Inc. <br/><br/> Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. Because all of the containers share the services of a single operating system kernel, they use fewer resources than virtual machines.",
    content: [
      {
        name: "Concept",
        intro: "",
        data: [
          {
            name: "What is a Container?",
            description:
              "Containers are lightweight packages of your application code together with dependencies such as specific versions of programming language runtimes and libraries required to run your software services. Containers make it easy to share CPU, memory, storage, and network resources at the operating systems level and offer a logical packaging mechanism in which applications can be abstracted from the environment in which they actually run. ",
          },
          {
            name: "Benefits of containers",
            description:
              "<ul><li><div><u>Separation of responsibility</u><br/>Containerization provides a clear separation of responsibility, as developers focus on application logic and dependencies, while IT operations teams can focus on deployment and management instead of application details such as specific software versions and configurations. </div></li><li><div><u>Workload portability</u><br/>Containers can run virtually anywhere, greatly easing development and deployment: on Linux, Windows, and Mac operating systems; on virtual machines or on physical servers; on a developer's machine or in data centres on-premises; and of course, in the public cloud. </div></li><li><div><u>Application isolation</u><br/>Containers virtualize CPU, memory, storage, and network resources at the operating system level, providing developers with a view of the OS logically isolated from other applications. </div></li></ul>",
          },
          {
            name: "Docker Image",
            description:
              "A Docker \"image\" behaves like a template from which consistent containers can be created. If Docker was a traditional virtual machine, the image could be likened to the ISO used to install your VM. This isn't a robust comparison, as Docker differs from VMs in terms of both concept and implementation, but it's a useful starting point nonetheless. <br/>Images define the initial filesystem state of new containers. They bundle your application's source code and its dependencies into a self-contained package that's ready to use with a container runtime. Within the image, filesystem content is represented as multiple independent layers.",
          },
          {
            name: "Docker Image Layers",
            description:
              "Layers are a result of the way Docker images are built. Each step in a Dockerfile creates a new \"layer\" that's essentially a diff of the filesystem changes since the last step. Metadata instructions such as LABEL and MAINTAINER do not create layers because they don't affect the filesystem.",
          },
          {
            name: "Dockerfile",
            description:
              "Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.",
          },
          {
            name: "Docker Swarm",
            description:
              "A Docker Swarm is a group of either physical or virtual machines that are running the Docker application and that have been configured to join together in a cluster. The activities of the cluster are controlled by a swarm manager, and machines that have joined the cluster are referred to as nodes.",
          },
        ],
      },
    ],
    examples: [],
    reference: [
      "https://www.docker.com/",
      "https://ragin.medium.com/docker-what-it-is-how-images-are-structured-docker-vs-vm-and-some-tips-part-1-d9686303590f",
    ],
  },
  {
    technology: "Datadog",
    docsLink: "https://docs.datadoghq.com/",
    intro:
      "Datadog is a SaaS-based monitoring and analytics platform for large-scale applications and infrastructure. Combining real-time metrics from servers, containers, databases, and applications with end-to-end tracing, Datadog delivers actionable alerts and powerful visualizations to provide full-stack observability. Datadog includes over 200 vendor-supported integrations and APM libraries for several languages.<br/><br/>Key features include: <ul><li>Log management</li><li>Infrastructure</li><li>Metrics</li><li>Monitoring and alerting</li><li>Application performance management (APM)</li><li>Synthetic monitoring</li><li>Continuous profiler</li><li>Continuous testing</li><li>CI Visibility</li><li>Database monitoring</li><li>Observability pipelines</li><li>Security dashboard</li></ul>",
    content: [
      {
        name: "Key features",
        intro: "",
        data: [
          {
            name: "Log management",
            description:
              "Logging the important parts of your system's operations is crucial for maintaining infrastructure health. Modern infrastructure has the capability to generate thousands of log events per minute. In this situation, you need to choose which logs to send to a log management solution, and which logs to archive. Filtering your logs before sending them, however, may lead to gaps in coverage or the accidental removal of valuable data. <br/>Datadog Log Management, also referred to as Datadog logs or logging, removes these limitations by decoupling log ingestion from indexing. This enables you to cost-effectively collect, process, archive, explore, and monitor all of your logs without limitations, also known as Logging without Limits. ",
          },
          {
            name: "Infrastructure",
            description:
              "<ul><li>Infrastructure List - See a list of all your hosts monitored by Datadog </li><li>Host Map - Visualize hosts together on one screen, with metrics made comprehensible through color and shape </li><li>Container Map - Visualize your containers together on one screen with customized groupings, filters, and metrics made comprehensible by color and shape. </li><li>Live Processes - Monitor your processes with real-time visibility of the most granular elements in a deployment.  </li><li>Generate Process Metrics - Generate global and percentile distribution metrics from your processes.  </li><li>Live Containers - Monitor the containers across your environment with real-time visibility  </li><li>Serverless - Bring together metrics, traces, and logs from your AWS Lambda functions running serverless applications into one view</li></ul>",
          },
          {
            name: "Metrics",
            description:
              "Metrics are numerical values that can track anything about your environment over time, from latency to error rates to user signups. It provides an overall picture of your system. You can use them to assess the health of your environment at a glance. Visualize how quickly users are loading your website, or the average memory consumption of your servers, for instance. Once you identify a problem, you can use logs and tracing to further troubleshoot. <br/><br/>Metrics that track system health come automatically through Datadog's integrations with more than 600 services. You can also track metrics that are specific to your business—also known as custom metrics. You can track things such as the number of user logins or user cart sizes to the frequency of your team's code commits. <br/><br/> In addition, metrics can help you adjust the scale of your environment to meet the demand from your customers. Knowing exactly how much you need to consume in resources can help you save money or improve performance.",
          },
          {
            name: "Monitoring and alerting",
            description:
              "Monitoring all of your infrastructure in one place wouldn't be complete without the ability to know when critical changes are occurring. Datadog gives you the ability to create monitors that actively check metrics, integration availability, network endpoints, and more. Configure monitors, notify your teams, and manage alerts at a glance on the Alerting platform.",
          },
          {
            name: "Application performance management (APM)",
            description:
              "Datadog Application Performance Monitoring (APM) gives deep visibility into your applications with out-of-the-box performance dashboards for web services, queues, and databases to monitor requests, errors, and latency. Distributed traces seamlessly correlate to browser sessions, logs, profiles, synthetic checks, network, processes, and infrastructure metrics across hosts, containers, proxies, and serverless functions. Navigate directly from investigating a slow trace to identifying the specific line of code causing performance bottlenecks with code hotspots.",
          },
          {
            name: "Synthetic monitoring",
            description:
              "Synthetic tests allow you to observe how your systems and applications are performing using simulated requests and actions from around the globe. Datadog tracks the performance of your webpages and APIs from the backend to the frontend, and at various network levels (HTTP, SSL, DNS, WebSocket, TCP, UDP, ICMP, and gRPC) in a controlled and stable way, alerting you about faulty behaviour such as regressions, broken features, high response times, and unexpected status codes.",
          },
          {
            name: "Continuous profiler",
            description:
              "Find CPU, memory, and IO bottlenecks, broken down by method name, class name, and line number, to significantly reduce end-user latency and infrastructure costs.",
          },
          {
            name: "Continuous testing",
            description:
              "Continuous Testing offers a set of tools that enable you to automate software testing for a product's entire lifecycle. By offering code-free, reliable end-to-end testing and seamless integrations with popular CI providers and collaboration tools, Continuous Testing helps you accelerate application development and ship high-quality features faster.",
          },
          {
            name: "CI Visibility",
            description:
              "Continuous Integration (CI) Visibility brings together information about CI test and pipeline results plus data about CI performance, trends, and reliability, all into one place. Not only does it provide developers with the ability to dig into the reasons for a test or pipeline failure, to monitor trends in test suite execution times, or to see the effect a given commit has on the pipeline, it also gives build engineers visibility into cross-organization CI health and trends in pipeline performance over time. <br/><br/>CI Visibility helps you troubleshoot test failures and broken builds, connecting the most pressing development outages to the commits that caused them. With the same libraries you use to trace application performance with APM, you can instrument your tests, generating traces from your testing frameworks as they execute in CI. Similarly, Datadog integrates with CI providers to gather pipeline metrics to track performance and results from the moment a commit enters the pipeline until it is ready to be deployed. Use the data aggregated over time to track trends in performance of tests and builds and to identify what is most important to fix. ",
          },
          {
            name: "Database monitoring",
            description:
              "Database Monitoring provides deep visibility into databases across all of your hosts. Dig into historical query performance metrics, explain plans, and host-level metrics all in one place, to understand the health and performance of your databases and troubleshoot issues as they arise.",
          },
          {
            name: "Observability pipelines",
            description:
              "Observability Pipelines is a monitoring solution built on Vector, an open source tool that enables you to monitor and manage all of your telemetry pipelines at scale. Vector is deployed as an aggregator within your infrastructure to collect, transform, and route all of your logs, metrics, and traces to any destination. <br/><br/>Add your Datadog API key to your Vector configuration to connect it to Observability Pipelines. Use Observability Pipelines to monitor your Vector pipelines and identify bottlenecks and latencies, fine-tune performance, monitor data delivery, and more. <br/>With Observability Pipelines, you can also: <ul><li>Control your data volume before routing to manage costs. </li><li>Route data anywhere to reduce vendor lock-in and simplify migrations. </li><li>Meet residency requirements and redact sensitive data to stay more compliant. </li><li>Enrich, structure, and transform your events to make them more useful. </li></ul><br/>Build performant and reliable data pipelines with complete visibility and simplified management using Observability Pipelines.",
          },
          {
            name: "Security dashboard",
            description:
              "Bring speed and scale to your production security operations. Datadog Security delivers real-time threat detection, and continuous configuration audits across applications, hosts, containers, and cloud infrastructure. Coupled with the greater Datadog observability platform, Datadog Security brings unprecedented integration between security and operations aligned to your organizations shared goals.",
          },
        ],
      },
    ],
    examples: [],
    reference: ["https://www.datadoghq.com/"],
  },
];
export default techContent;
